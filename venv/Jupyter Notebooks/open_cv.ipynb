{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8t0bvcSKsNd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvXvxaFnK-L3"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"dua_lipa.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "uCkoNpmuMrGm",
    "outputId": "73f81958-dfc1-40dc-f3ae-0de4ab178119"
   },
   "outputs": [],
   "source": [
    "cv2.imshow('dua_lipa', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICvawglhMw__"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 815, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('dua_lipa.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greyscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('dua_lipa', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "grey_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #== cv2.imread(\"dua_lipa.jpg\",0)\n",
    "\n",
    "cv2.imshow(\"Greyscaled Image\", grey_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling, re-scaling, interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_scaled = cv2.resize(img, dsize=None, fx=0.5, fy=0.5)\n",
    "cv2.imshow('Linear interpolation', img_scaled)\n",
    "\n",
    "image_doubled = cv2.resize(img, dsize=None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "cv2.imshow('Cubic Interpolatio', image_doubled)\n",
    "\n",
    "image_skewed = cv2.resize(img, dsize=(400,300), interpolation=cv2.INTER_AREA)\n",
    "cv2.imshow(\"Sweved\", image_skewed)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222, 815)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = img.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row, start_column = int(height * .25), int(width * .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_row, end_column = int(height * 0.75), int(width * 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = img[start_row: end_row, start_column:end_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('cropped image', cropped)\n",
    "cv2.imshow('original image', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407.5, 611.0)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center = (width / 2), (height / 2)\n",
    "center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotatiom_matrix = cv2.getRotationMatrix2D(center, angle, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate = cv2.warpAffine(img, rotatiom_matrix, (width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('rotate', rotate)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#karnel filter mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_scaled \n",
    "gaussian =cv2.GaussianBlur(img, (7,7), 0)\n",
    "cv2.imshow('Gaussian Blurr', gaussian)\n",
    "median = cv2.medianBlur(img, 5)\n",
    "cv2.imshow('Median Blurr', median)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b-w\n",
    "img = cv2.imread(\"dua_lipa.jpg\", 0)\n",
    "#resize\n",
    "smaller_img = cv2.resize(img, dsize=None, fx=0.5, fy=0.5)\n",
    "#edge  it means anything below 60 -black, abowe 120 -white\n",
    "canny = cv2.Canny(smaller_img, 60, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Canny', canny)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# live skeath with webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketch(image):\n",
    "    img_grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_gray_blur = cv2.GaussianBlur(img_grey, (5,5), 0 )\n",
    "    threshold_value1 = 10\n",
    "    threshold_value2 = 70\n",
    "    \n",
    "    canny_edges = cv2.Canny(img_gray_blur, threshold_value1, threshold_value2)\n",
    "    ret, mask = cv2.threshold(canny_edges, 70, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow('Our live sketch', sketch(frame))\n",
    "    #close camera after pressing enter\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "    \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face detection and framing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clasifier = cv2.CascadeClassifier('../../venv/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"dua_lipa.jpg\")\n",
    "#resize\n",
    "image = cv2.resize(img, dsize=None, fx=0.5, fy=0.5)\n",
    "img_grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "scale_factor = 1.3\n",
    "min_neighbours = 5\n",
    "face = face_clasifier.detectMultiScale(img_grey, scale_factor, min_neighbours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "if faces is ():\n",
    "    print('No faces Found')\n",
    "    \n",
    "for (x,y,w,h) in faces:\n",
    "    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    #roi_gray = gray[y:y+h, x:x+w]\n",
    "    #roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Life face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clasifier = cv2.CascadeClassifier('../../venv/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection(frame):\n",
    "    scale_factor = 1.3\n",
    "    min_neighbours = 5\n",
    "    \n",
    "    grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face = face_clasifier.detectMultiScale(grey, scale_factor, min_neighbours,)\n",
    "    if face is ():\n",
    "        return frame\n",
    "    \n",
    "    for (x,y,w,h) in face:\n",
    "        rec = cv2.rectangle(frame ,(x,y),(x+w,y+h),(0,250,0),2)\n",
    "        return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow('Our live sketch', face_detection(frame))\n",
    "    #close camera after pressing enter\n",
    "    if cv2.waitKey(1) == 13:\n",
    "        break\n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "open_cv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
